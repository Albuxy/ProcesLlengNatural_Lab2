{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"P2- Lab2_195310 _194994_195003 .ipynb","provenance":[],"collapsed_sections":["aSRPrKrHMloB","388GbQDX9GHv","9SdVtSuZ9k2Q","0v3SHU5zyMSj"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aSRPrKrHMloB"},"source":["# Load dataset"]},{"cell_type":"markdown","metadata":{"id":"r8WWNznbZy48"},"source":["**Permit the access to the Shared Drives for import the dataset de dev-v2.0.sjon**"]},{"cell_type":"code","metadata":{"id":"GJfk_ssCYqUg","colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"status":"error","timestamp":1606245146189,"user_tz":-60,"elapsed":327209,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"3eaf48de-e088-4a43-f807-97eb6bfe778b"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","#The path of our shared drive in the folder named Proc Lleng Natural\n","data_path = '/content/drive/Shared drives/Proc Lleng Natural/Project_data/vcr1annots'\n","\n","!ls '/content/drive/Shared drives/Proc Lleng Natural/Project_data/vcr1annots'"],"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-391d065ddd38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#The path of our shared drive in the folder named Proc Lleng Natural\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    249\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ab4kOqUc9S6j"},"source":["import json\n","import pandas as pd\n","\n","#Load the dataset \n","train_data = data_path + 'dev-v2.0.json'\n","\n","with open(train_data, 'r') as f:\n","  data = json.load(f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FChVHOGYMA8Q"},"source":["# Part 1"]},{"cell_type":"markdown","metadata":{"id":"rDOoxXR-uQBc"},"source":["**1. Select 100 paragraphs, they are called contexts in the SQuAD dataset**"]},{"cell_type":"code","metadata":{"id":"EyvfqkbsaCPE","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1604587538675,"user_tz":-60,"elapsed":1624,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"dce1d415-7d82-4bc0-96b3-86e729880b66"},"source":["paragraphs = list()\n"," \n","#Create a list with paragraphs \"context\"\n","for i in range(len(data['data'])):\n","  for j in range(len(data['data'][i]['paragraphs'])):\n","    if(len(paragraphs) < 100):\n","      paragraphs.append(data['data'][i]['paragraphs'][j]['context'])\n","\n","#See the results\n","context_data = pd.DataFrame(paragraphs)\n","context_data\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The Norman dynasty had a major political, cult...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The English name \"Normans\" comes from the Fren...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>In the course of the 10th century, the initial...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Before Rollo's arrival, its populations did no...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Many locals and tourists frequent the southern...</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>\"Southern California\" is not a formal geograph...</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Though there is no official definition for the...</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>Subsequently, Californios (dissatisfied with i...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>In 1900, the Los Angeles Times defined souther...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                    0\n","0   The Normans (Norman: Nourmands; French: Norman...\n","1   The Norman dynasty had a major political, cult...\n","2   The English name \"Normans\" comes from the Fren...\n","3   In the course of the 10th century, the initial...\n","4   Before Rollo's arrival, its populations did no...\n","..                                                ...\n","95  Many locals and tourists frequent the southern...\n","96  \"Southern California\" is not a formal geograph...\n","97  Though there is no official definition for the...\n","98  Subsequently, Californios (dissatisfied with i...\n","99  In 1900, the Los Angeles Times defined souther...\n","\n","[100 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"pbvQK1XdwloD"},"source":["**2. Create a list of answers for each paragraph**"]},{"cell_type":"code","metadata":{"id":"pKAtQ5KZxGQM","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1604587538675,"user_tz":-60,"elapsed":1616,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"346530c7-7caa-4d65-8bab-e198cc20cfac"},"source":["#Generate the list with the answers for each context\n","list_answers = list() \n","paragraphs = list()\n"," \n","#Create a list with paragraphs \"context\"\n","for i in range(len(data['data'])):\n","  for j in range(len(data['data'][i]['paragraphs'])):\n","    if(len(paragraphs) < 100):\n","      paragraphs.append(data['data'][i]['paragraphs'][j]['context'])\n","      for k in range(len(data['data'][i]['paragraphs'][j]['qas'])):\n","        for l in range(len(data['data'][i]['paragraphs'][j]['qas'][k]['answers'])):\n","          list_answers.append(data['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text'])\n","          break\n","\n","#See the results\n","answers_data = pd.DataFrame(list_answers)\n","answers_data      "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>France</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10th and 11th centuries</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Denmark, Iceland and Norway</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Rollo</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10th century</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>351</th>\n","      <td>Los Angeles Times</td>\n","    </tr>\n","    <tr>\n","      <th>352</th>\n","      <td>1900</td>\n","    </tr>\n","    <tr>\n","      <th>353</th>\n","      <td>1999</td>\n","    </tr>\n","    <tr>\n","      <th>354</th>\n","      <td>Imperial</td>\n","    </tr>\n","    <tr>\n","      <th>355</th>\n","      <td>seven</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>356 rows × 1 columns</p>\n","</div>"],"text/plain":["                               0\n","0                         France\n","1        10th and 11th centuries\n","2    Denmark, Iceland and Norway\n","3                          Rollo\n","4                   10th century\n","..                           ...\n","351            Los Angeles Times\n","352                         1900\n","353                         1999\n","354                     Imperial\n","355                        seven\n","\n","[356 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"388GbQDX9GHv"},"source":["### Spicy Model"]},{"cell_type":"markdown","metadata":{"id":"xHCSRC7VkZsX"},"source":["**3. Run the model for named entity recognition/concept extraction and extract\n","entities in each paragraph**\n"]},{"cell_type":"code","metadata":{"id":"ngLYhACVq1KA","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1604587542945,"user_tz":-60,"elapsed":5878,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"697b009c-7b63-4e9c-e26b-b5b106472a1e"},"source":["import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","entities = list()\n","\n","for i in range(len(paragraphs)):\n","  doc = nlp(paragraphs[i])\n","  for ent in doc.ents:\n","    if ent.text not in entities:\n","      entities.append(ent.text) \n","\n","#See the results\n","entities_data = pd.DataFrame(entities)\n","entities_data      "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Normans</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Norman</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Nourmands</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>French</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Latin</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>600</th>\n","      <td>1900</td>\n","    </tr>\n","    <tr>\n","      <th>601</th>\n","      <td>the Los Angeles Times</td>\n","    </tr>\n","    <tr>\n","      <th>602</th>\n","      <td>1999</td>\n","    </tr>\n","    <tr>\n","      <th>603</th>\n","      <td>Times</td>\n","    </tr>\n","    <tr>\n","      <th>604</th>\n","      <td>newer county</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>605 rows × 1 columns</p>\n","</div>"],"text/plain":["                         0\n","0                  Normans\n","1                   Norman\n","2                Nourmands\n","3                   French\n","4                    Latin\n","..                     ...\n","600                   1900\n","601  the Los Angeles Times\n","602                   1999\n","603                  Times\n","604           newer county\n","\n","[605 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"BEzbRium7w9-"},"source":["**4. For each paragraph find out the number of answers that are in a list of\n","extracted entities and sum up these numbers**"]},{"cell_type":"code","metadata":{"id":"Pw1XK5STOR-W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604587542946,"user_tz":-60,"elapsed":5871,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"575199e7-2e88-4a33-eb4b-164b9b415148"},"source":["number_entities_context = 0\n","\n","for i in range(len(list_answers)):\n","  for j in range(len(entities)):\n","    if (list_answers[i] == entities[j]):\n","     number_entities_context = number_entities_context + 1\n","\n","print(\"The number of entities in the answers are:\", number_entities_context)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The number of entities in the answers are: 106\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ik5bE2i4OSju"},"source":["**5. Return a single value that is the proportion of answers that are entities.**"]},{"cell_type":"code","metadata":{"id":"cuJBPTti0337","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604587542946,"user_tz":-60,"elapsed":5864,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"b0591e2f-1efb-4cea-c1a1-9897169f6064"},"source":["proportion_of_entities = 0\n","\n","proportion_of_entities = number_entities_context / len(list_answers)\n","\n","print(\"The proportion is: \", proportion_of_entities)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The proportion is:  0.29775280898876405\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9SdVtSuZ9k2Q"},"source":["### Nltk Model"]},{"cell_type":"markdown","metadata":{"id":"TQ7SiZv294EA"},"source":["**3. Run the model for named entity recognition/concept extraction and extract entities in each paragraph**"]},{"cell_type":"code","metadata":{"id":"Lav4qq43-AFX","colab":{"base_uri":"https://localhost:8080/","height":606},"executionInfo":{"status":"ok","timestamp":1604587544603,"user_tz":-60,"elapsed":7513,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"924b53ec-3d63-4838-dcd7-62e596dedd6a"},"source":["import nltk\n","\n","import string\n","nltk.download('maxent_ne_chunker')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('punkt')\n","nltk.download('words')\n","\n","entities_nltk = list()\n","\n","def ntlk_entity(text):\n","    for chunk in chunks: \n","      if hasattr(chunk,'label'):\n","        entities_nltk.append(' '.join(c[0] for c in chunk))\n","\n","for i in range(len(paragraphs)):\n","  words = nltk.word_tokenize(paragraphs[i])\n","  pos_tags = nltk.pos_tag(words)\n","  chunks = nltk.ne_chunk(pos_tags, binary=True)\n","  ntlk_entity(chunks)\n","\n","#See the results\n","entities_df = pd.DataFrame(entities_nltk)\n","entities_df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Normans</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Norman</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>French</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Normandy</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>France</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>775</th>\n","      <td>Riverside</td>\n","    </tr>\n","    <tr>\n","      <th>776</th>\n","      <td>San Diego</td>\n","    </tr>\n","    <tr>\n","      <th>777</th>\n","      <td>Ventura</td>\n","    </tr>\n","    <tr>\n","      <th>778</th>\n","      <td>Santa Barbara</td>\n","    </tr>\n","    <tr>\n","      <th>779</th>\n","      <td>Times</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>780 rows × 1 columns</p>\n","</div>"],"text/plain":["                 0\n","0          Normans\n","1           Norman\n","2           French\n","3         Normandy\n","4           France\n","..             ...\n","775      Riverside\n","776      San Diego\n","777        Ventura\n","778  Santa Barbara\n","779          Times\n","\n","[780 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"uG0e2BK1-DAZ"},"source":["**4. For each paragraph find out the number of answers that are in a list of\n","extracted entities and sum up these numbers**"]},{"cell_type":"code","metadata":{"id":"sPnnt912GiQ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604587544604,"user_tz":-60,"elapsed":7506,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"cbed2f6d-be8f-4d6e-eb2a-031767b9f48b"},"source":["number_entities_context = 0\n","\n","for i in range(len(list_answers)):\n","  for j in range(len(entities_nltk)):\n","    if (list_answers[i] == entities_nltk[j]):\n","     number_entities_context = number_entities_context + 1\n","\n","print(\"The number of entities in the answers are:\", number_entities_context)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The number of entities in the answers are: 273\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Aco41Ov-HL1"},"source":["**5. Return a single value that is the proportion of answers that are entities.**"]},{"cell_type":"code","metadata":{"id":"sFEf92gR-IGt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604587544605,"user_tz":-60,"elapsed":7500,"user":{"displayName":"ROGER MARTIN - PERO","photoUrl":"","userId":"02375045331010606322"}},"outputId":"9d63e82c-2c15-4fbd-9e48-40f6842ede18"},"source":["proportion_of_entities = 0\n","\n","proportion_of_entities = number_entities_context / len(list_answers)\n","\n","print(\"The proportion is: \", proportion_of_entities)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The proportion is:  0.7668539325842697\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rFpwFYYJq69I"},"source":["# Part 2"]},{"cell_type":"markdown","metadata":{"id":"Y1rSkAcJq_fu"},"source":["**1. Select 100 paragraphs that have at least one corresponding “who” question.**"]},{"cell_type":"code","metadata":{"id":"WXEgqn6G3HcL","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1604592954072,"user_tz":-60,"elapsed":3183,"user":{"displayName":"ALBA TORRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsF_1itCABg5QEm4zb86DpqTpvnYPheWgkyg4j=s64","userId":"14532359862838600745"}},"outputId":"d849ea76-1c7f-4f10-d3e9-4e1703768307"},"source":["paragraphs_who_list = list()\n","\n","#Create a list with paragraphs \"context\"\n","for i in range(len(data['data'])):\n","  for j in range(len(data['data'][i]['paragraphs'])):\n","    if data['data'][i]['paragraphs'] not in paragraphs_who_list:\n","      if len(paragraphs_who_list) < 100:\n","        for k in range(len(data['data'][i]['paragraphs'][j]['qas'])):\n","          if data['data'][i]['paragraphs'][j]['qas'][k]['question'].startswith('Who'):\n","            paragraphs_who_list.append(data['data'][i]['paragraphs'][j]['context'])\n","            break\n","\n","#See the results\n","context_data = pd.DataFrame(paragraphs_who_list)\n","display(context_data)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The Norman dynasty had a major political, cult...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>In the course of the 10th century, the initial...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Before Rollo's arrival, its populations did no...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The Normans thereafter adopted the growing feu...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>In 1891 Scottish chemist James Dewar was able ...</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>By the late 19th century scientists realized t...</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Paleoclimatologists measure the ratio of oxyge...</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>Hyperbaric (high-pressure) medicine uses speci...</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>People who climb mountains or fly in non-press...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                    0\n","0   The Normans (Norman: Nourmands; French: Norman...\n","1   The Norman dynasty had a major political, cult...\n","2   In the course of the 10th century, the initial...\n","3   Before Rollo's arrival, its populations did no...\n","4   The Normans thereafter adopted the growing feu...\n","..                                                ...\n","95  In 1891 Scottish chemist James Dewar was able ...\n","96  By the late 19th century scientists realized t...\n","97  Paleoclimatologists measure the ratio of oxyge...\n","98  Hyperbaric (high-pressure) medicine uses speci...\n","99  People who climb mountains or fly in non-press...\n","\n","[100 rows x 1 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"nvcrkLa8rHCN"},"source":["**2. Create a list of answers only to “who” questions for each paragraph.**"]},{"cell_type":"code","metadata":{"id":"06GS8mb68vPl","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1604592963637,"user_tz":-60,"elapsed":931,"user":{"displayName":"ALBA TORRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsF_1itCABg5QEm4zb86DpqTpvnYPheWgkyg4j=s64","userId":"14532359862838600745"}},"outputId":"f8fa64e5-63ab-4ccb-eb06-acb8b0e2178e"},"source":["answers_list = list()\n","\n","#Create a list with paragraphs \"context\"\n","for i in range(len(data['data'])):\n","  for j in range(len(data['data'][i]['paragraphs'])):\n","        for k in range(len(data['data'][i]['paragraphs'][j]['qas'])):\n","          if data['data'][i]['paragraphs'][j]['qas'][k]['question'].startswith('Who'):\n","            for l in range(len(data['data'][i]['paragraphs'][j]['qas'][k]['answers'])):\n","              if data['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text'] not in answers_list:\n","                answers_list.append(data['data'][i]['paragraphs'][j]['qas'][k]['answers'][l]['text'])\n","            break\n","\n","\n","#See the results\n","context_data = pd.DataFrame(answers_list)\n","display(context_data)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Rollo</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>William the Conqueror</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>King Charles III</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Seljuk Turks</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>the Pechenegs, the Bulgars, and especially the...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>484</th>\n","      <td>Albert Einstein</td>\n","    </tr>\n","    <tr>\n","      <th>485</th>\n","      <td>Isaac Newton</td>\n","    </tr>\n","    <tr>\n","      <th>486</th>\n","      <td>Galileo</td>\n","    </tr>\n","    <tr>\n","      <th>487</th>\n","      <td>Henry Cavendish</td>\n","    </tr>\n","    <tr>\n","      <th>488</th>\n","      <td>James Clerk Maxwell</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>489 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                     0\n","0                                                Rollo\n","1                                William the Conqueror\n","2                                     King Charles III\n","3                                         Seljuk Turks\n","4    the Pechenegs, the Bulgars, and especially the...\n","..                                                 ...\n","484                                    Albert Einstein\n","485                                       Isaac Newton\n","486                                            Galileo\n","487                                    Henry Cavendish\n","488                                James Clerk Maxwell\n","\n","[489 rows x 1 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"pub2mLBZrLg7"},"source":["**3-4. Apply syntactic parsing for each paragraph and find answers in parse trees and assign a syntactic tag for each answer.**"]},{"cell_type":"code","metadata":{"id":"XgyBxaZaCUen"},"source":["import spacy\n","from spacy import displacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","parsing = list()\n","\n","#Apply syntactic parsing for the list of paragraphs\n","for i in range(len(paragraphs_who_list)):\n","  doc = nlp(paragraphs_who_list[i])\n","  sentence_spans = list(doc.sents)\n","  displacy.render(sentence_spans, style = 'dep', jupyter = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPPZOFtIrcJS"},"source":["**5. For each paragraph find out the number of “who” answers that appear in a position of the grammatical subject (nsubj) and sum up these numbers.**"]},{"cell_type":"code","metadata":{"id":"rJ-gd4XirfWX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604591153638,"user_tz":-60,"elapsed":7768,"user":{"displayName":"ALBA TORRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsF_1itCABg5QEm4zb86DpqTpvnYPheWgkyg4j=s64","userId":"14532359862838600745"}},"outputId":"a5c0e30b-13da-446e-9a51-9c282ccb2204"},"source":["import spacy\n","from spacy import displacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","counter_nsubj = 0\n","\n","for i in range(len(paragraphs_who_list)):\n","  doc = nlp(paragraphs_who_list[i])\n","  \n","  for chunk in doc.noun_chunks:\n","    for answer in answers_list:\n","      if chunk.text == answer:\n","        if chunk.root.dep_ == 'nsubj':\n","          counter_nsubj = counter_nsubj + 1\n","\n","print(\"The number of 'who' answers that are subjects (nsubj):\", counter_nsubj)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The number of 'who' that appear in a position of the grammatical subject (nsubj): 38\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GCkBIG6irflu"},"source":["**6. For each paragraph find out the number of “who” answers that appear in the position of the grammatical object (podj or dobj) and sum up these numbers.**"]},{"cell_type":"code","metadata":{"id":"VvfzdjkHrihQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604591165636,"user_tz":-60,"elapsed":7506,"user":{"displayName":"ALBA TORRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsF_1itCABg5QEm4zb86DpqTpvnYPheWgkyg4j=s64","userId":"14532359862838600745"}},"outputId":"78420803-fd5d-457a-b7f5-43889d33d74a"},"source":["import spacy\n","from spacy import displacy\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","counter_podj_dobj = 0\n","\n","for i in range(len(paragraphs_who_list)):\n","  doc = nlp(paragraphs_who_list[i])\n","  \n","  for chunk in doc.noun_chunks:\n","    for answer in answers_list:\n","      if chunk.text == answer:\n","        if chunk.root.dep_ == 'pobj' or chunk.root.dep_ == 'dobj':\n","          counter_podj_dobj = counter_podj_dobj + 1\n","\n","print(\"The number of 'who' answers that are objects (podj or dobj ):\", counter_podj_dobj)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The number of 'who' that appear in a position of the grammatical subject (podj or dobj ): 42\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H_23BrOsri9a"},"source":["**7. Return two values that are the proportion of answers that are subjects and the proportion of answers that are objects.**"]},{"cell_type":"code","metadata":{"id":"SgO8kugYrnMH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604591204339,"user_tz":-60,"elapsed":1303,"user":{"displayName":"ALBA TORRA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsF_1itCABg5QEm4zb86DpqTpvnYPheWgkyg4j=s64","userId":"14532359862838600745"}},"outputId":"972c3274-c71c-445b-c515-bc3789830a65"},"source":["proportion_of_answers__nsubj = 0\n","proportion_of_answers__podj_dobj= 0\n","\n","proportion_of_answers__nsubj = counter_nsubj / len(answers_list)\n","\n","proportion_of_answers__podj_dobj = counter_podj_dobj / len(answers_list)\n","\n","\n","print(\"The proportion of subjects: \", proportion_of_answers__nsubj)\n","print(\"The proportion of objects: \", proportion_of_answers__podj_dobj)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The proportion of subjects:  0.07770961145194274\n","The proportion of objects:  0.08588957055214724\n"],"name":"stdout"}]}]}